{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with Cosine Annealing + Cutout + Gradient Clipping\n",
    "Use training procedure outlined in [source](https://github.com/D-X-Y/NAS-Projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "os.chdir(PATH)\n",
    "from conf import settings\n",
    "from utils.model_utils import get_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    \"\"\"\n",
    "    Takes as input a string of args\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-net', type=str, required=True, help='net type')\n",
    "    parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n",
    "    parser.add_argument('-w', type=int, default=2, help='number of workers for dataloader')\n",
    "    parser.add_argument('-b', type=int, default=32, help='batch size for dataloader')\n",
    "    parser.add_argument('-s', type=bool, default=True, help='whether shuffle the dataset')\n",
    "    parser.add_argument('-warm', type=int, default=1, help='warm up training phase')\n",
    "    parser.add_argument('-lr', type=float, default=0.1, help='initial learning rate')\n",
    "    return parser.parse_args(args.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = '-net nasnet -b 64 -lr 0.025'\n",
    "args = parse_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = get_network(args, use_gpu=args.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('{name}(length={length})'.format(name=self.__class__.__name__, **self.__dict__))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "        mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img *= mask\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_train():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(settings.CIFAR100_TRAIN_MEAN, settings.CIFAR100_TRAIN_STD),\n",
    "        Cutout(16)\n",
    "    ])\n",
    "    return transform_train\n",
    "    \n",
    "    \n",
    "def get_transform_test():\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(settings.CIFAR100_TRAIN_MEAN, settings.CIFAR100_TRAIN_STD),\n",
    "    ])\n",
    "    return transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 training dataset\n",
    "        std: std of cifar100 training dataset\n",
    "        path: path to cifar100 training python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle \n",
    "    Returns: train_data_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = get_transform_train()\n",
    "    cifar100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    cifar100_training_loader = DataLoader(\n",
    "        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_training_loader\n",
    "\n",
    "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 test dataset\n",
    "        std: std of cifar100 test dataset\n",
    "        path: path to cifar100 test python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle \n",
    "    Returns: cifar100_test_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_test = get_transform_test()\n",
    "    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    cifar100_test_loader = DataLoader(\n",
    "        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing:\n",
    "cifar100_training_loader = get_training_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=args.w,\n",
    "    batch_size=args.b,\n",
    "    shuffle=args.s\n",
    ")\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=args.w,\n",
    "    batch_size=args.b,\n",
    "    shuffle=args.s\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), args.lr, momentum=0.9, weight_decay=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Scheduler\n",
    "Use cosine annealing. Paper uses a crazy amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 600\n",
    "lr_min = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                       float(epochs), \n",
    "                                                       eta_min=float(lr_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(settings.LOG_DIR):\n",
    "    os.mkdir(settings.LOG_DIR)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\n",
    "        settings.LOG_DIR, args.net, settings.TIME_NOW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Checkpoint Folder to Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint folder to save model\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    net.train()\n",
    "    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n",
    "        \n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def eval_training(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in cifar100_test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "    #add informations to tensorboard\n",
    "    writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n",
    "    writer.add_scalar('Test/Accuracy', correct.float() / len(cifar100_test_loader.dataset), epoch)\n",
    "\n",
    "    return correct.float() / len(cifar100_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "for epoch in range(1, settings.EPOCH):\n",
    "    train(epoch)\n",
    "    train_scheduler.step()\n",
    "    acc = eval_training(epoch)\n",
    "    \n",
    "    send_message('Epoch ' + str(epoch) + ' complete. Accuracy: ' + str(acc.item()), channel=args.net)\n",
    "\n",
    "    #start to save best performance model after learning rate decay to 0.01 \n",
    "    if epoch > settings.MILESTONES[1] and best_acc < acc:\n",
    "        torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type='best'))\n",
    "        best_acc = acc\n",
    "        continue\n",
    "\n",
    "    if not epoch % settings.SAVE_EPOCH:\n",
    "        torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type='regular'))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
