{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate nasnet\n",
    "Evaluate nasnet using micronet challenge [scoring guidelines](https://micronet-challenge.github.io/scoring_and_submission.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "os.chdir(PATH)\n",
    "from conf import settings\n",
    "from utils.model_utils import get_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    \"\"\"\n",
    "    Takes as input a string of args\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-net', type=str, required=True, help='net type')\n",
    "    parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n",
    "    parser.add_argument('-w', type=int, default=2, help='number of workers for dataloader')\n",
    "    parser.add_argument('-b', type=int, default=32, help='batch size for dataloader')\n",
    "    parser.add_argument('-s', type=bool, default=True, help='whether shuffle the dataset')\n",
    "    parser.add_argument('-warm', type=int, default=1, help='warm up training phase')\n",
    "    parser.add_argument('-lr', type=float, default=0.1, help='initial learning rate')\n",
    "    return parser.parse_args(args.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = '-net nasnet -b 32 -lr 0.025'\n",
    "args = parse_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_network(args, use_gpu=args.gpu)\n",
    "\n",
    "# load model from checkpoint\n",
    "CHECKPOINT_PATH = '/root/UCLA-MAS-thesis/checkpoint/nasnet/nasnet-595-best.pth'\n",
    "net.load_state_dict(torch.load(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_transform_test():\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(settings.CIFAR100_TRAIN_MEAN, settings.CIFAR100_TRAIN_STD),\n",
    "    ])\n",
    "    return transform_test\n",
    "\n",
    "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 test dataset\n",
    "        std: std of cifar100 test dataset\n",
    "        path: path to cifar100 test python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle \n",
    "    Returns: cifar100_test_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_test = get_transform_test()\n",
    "    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    cifar100_test_loader = DataLoader(\n",
    "        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_test_loader\n",
    "\n",
    "\n",
    "# get the cifar100 test dataset\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=args.w,\n",
    "    batch_size=args.b,\n",
    "    shuffle=args.s\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy\n",
    "Accuracy should be above 80 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0.0\n",
    "\n",
    "for (images, labels) in cifar100_test_loader:\n",
    "    images = Variable(images)\n",
    "    labels = Variable(labels)\n",
    "\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    outputs = net(images)\n",
    "    _, preds = outputs.max(1)\n",
    "    correct += preds.eq(labels).sum()\n",
    "\n",
    "acc = correct.float() / len(cifar100_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for nasnet: 81.2800%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for ' + args.net + ': ' + '{0:.4%}'.format(acc.item()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Storage\n",
    "The model performs no quantization, so we use freebie quantization and divide the total number of parameters by 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasnet has 5221624 total parameters.\n",
      "nasnet achieves a parameter score of 0.07153\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(args.net +  ' has ' + '{0}'.format(total_params) + ' total parameters.')\n",
    "\n",
    "total_params_freebie = total_params / 2\n",
    "benchmark_params = 36.5e6\n",
    "parameter_score = total_params_freebie / benchmark_params\n",
    "print(args.net + ' achieves a parameter score of ' + '{0:.4}'.format(parameter_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Operations\n",
    "The model performs no quantization, so we use freebie quantization and divide the total number of multiplies by 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_output_size(image_size, filter_size, padding, stride):\n",
    "    \"\"\"Calculates the output size of convolution.\n",
    "    The input, filter and the strides are assumed to be square.\n",
    "    Arguments:\n",
    "    image_size: int, Dimensions of the input image (square assumed).\n",
    "    filter_size: int, Dimensions of the kernel (square assumed).\n",
    "    padding: str, padding added to the input image. 'same' or 'valid'\n",
    "    stride: int, stride with which the kernel is applied (square assumed).\n",
    "    Returns:\n",
    "    int, output size.\n",
    "    \"\"\"\n",
    "    if padding == 'same':\n",
    "        pad = filter_size // 2\n",
    "    elif padding == 'valid':\n",
    "        pad = 0\n",
    "    else:\n",
    "        raise NotImplementedError('Padding: %s should be `same` or `valid`.'\n",
    "                              % padding)\n",
    "    out_size = np.ceil((image_size - filter_size + 1. + 2 * pad) / stride)\n",
    "    return int(out_size)\n",
    "\n",
    "def count_Conv2d(conv_layer, x):\n",
    "    \"\"\"\n",
    "    Calculates the number of mults, adds\n",
    "    for a Conv2d pytorch module, given an input x.\n",
    "    Assume no sparsity and same padding.\n",
    "    \"\"\"\n",
    "    out_shape = conv_layer(x).shape\n",
    "    input_size = x.shape[2]\n",
    "    k_size, stride = conv_layer.kernel_size[0], conv_layer.stride[0]\n",
    "    c_in, c_out = conv_layer.in_channels, conv_layer.out_channels\n",
    "    padding = 'same'\n",
    "    \n",
    "    flop_mults = flop_adds = 0\n",
    "    \n",
    "    # Each application of the kernel can be thought as a dot product between\n",
    "    # the flattened kernel and patches of the image.\n",
    "    vector_length = (k_size * k_size * c_in)\n",
    "\n",
    "    # Number of elements in the output is OUT_SIZE * OUT_SIZE * OUT_CHANNEL\n",
    "    n_output_elements = get_conv_output_size(input_size, k_size, padding,\n",
    "                                             stride) ** 2 * c_out\n",
    "    # Each output is the product of a one dot product. Dot product of two\n",
    "    # vectors of size n needs n multiplications and n - 1 additions.\n",
    "    flop_mults += vector_length * n_output_elements\n",
    "    flop_adds += (vector_length - 1) * n_output_elements\n",
    "    \n",
    "    try:\n",
    "        # if bias has a shape, continue\n",
    "        conv_layer.bias.shape\n",
    "        # If we have bias we need one more addition per dot product.\n",
    "        flop_adds += n_output_elements\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # make sure the calculated number of output elements equals the actual\n",
    "    assert np.prod(out_shape) == n_output_elements\n",
    "\n",
    "    return flop_mults, flop_adds\n",
    "\n",
    "def count_FullyConnected(fc_layer, x):\n",
    "    \"\"\"\n",
    "    Calculates the number of mults, adds\n",
    "    for a Linear pytorch module, given an input x.\n",
    "    Assume no sparsity.\n",
    "    \"\"\"\n",
    "    input_size = x.shape[1]\n",
    "    c_in, c_out = fc_layer.in_features, fc_layer.out_features\n",
    "\n",
    "    \n",
    "    flop_mults = flop_adds = 0    \n",
    "    flop_mults += c_in * c_out\n",
    "    # We have one less addition than the number of multiplications per output\n",
    "    # channel.\n",
    "    flop_adds += (c_in - 1) * c_out\n",
    "\n",
    "    try:\n",
    "        # if bias has a shape, continue\n",
    "        fc_layer.bias.shape\n",
    "        # If we have bias we need one more addition per dot product.\n",
    "        flop_adds += c_out\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return flop_mults, flop_adds\n",
    "\n",
    "def countReLU(x):\n",
    "    # For the purposes of the \"freebie\" quantization scoring, ReLUs can be\n",
    "    # assumed to be performed on 16-bit inputs. Thus, we track them as\n",
    "    # multiplications in our accounting, which can also be assumed to be\n",
    "    # performed on reduced precision inputs.\n",
    "    flop_adds = 0\n",
    "    flop_mults = np.prod(list(x.shape))\n",
    "    return flop_mults, flop_adds\n",
    "\n",
    "def count_AvgPool2d(pool_layer, x):\n",
    "    \"\"\"\n",
    "    Calculates the number of mults, adds\n",
    "    for an AvgPool2d pytorch module, given an input x.\n",
    "    \"\"\"\n",
    "    n_channels = x.shape[1]\n",
    "    y = pool_layer(x)\n",
    "    output_size = y.shape[2]\n",
    "    kernel_size = pool_layer.kernel_size\n",
    "    stride = pool_layer.stride\n",
    "    padding = pool_layer.padding\n",
    "    \n",
    "    flop_mults = flop_adds = 0\n",
    "    # we perform averages of size kernel_size * kernel_size\n",
    "    flop_adds += output_size * output_size * (filter_size * filter_size - 1) * n_channels\n",
    "    # For each output channel we will make a division.\n",
    "    flop_mults += output_size * output_size * n_channels\n",
    "    \n",
    "    return flop_mults, flop_adds\n",
    "\n",
    "def count_AdaptiveAvgPool2d(pool_layer, x):\n",
    "    \"\"\"\n",
    "    Calculates the number of mults, adds\n",
    "    for an AdaptiveAvgPool2d pytorch module, given an input x.\n",
    "    Only implemented for output_size = 1\n",
    "    \"\"\"\n",
    "    n_channels = x.shape[1]\n",
    "    input_size = x.shape[2]\n",
    "    y = pool_layer(x)\n",
    "    output_size = y.shape[2]\n",
    "    # only implemented for output size = 1\n",
    "    assert output_size == 1\n",
    "    stencil_size = (input_size+output_size-1) // output_size\n",
    "    \n",
    "    flop_mults = flop_adds = 0\n",
    "    # we perform averages of size input_size * input_size for each channel for output_size = 1\n",
    "    flop_adds += (input_size * input_size - 1) * n_channels\n",
    "    # For each output channel we will make a division.\n",
    "    flop_mults += output_size * output_size * n_channels\n",
    "    \n",
    "    return flop_mults, flop_adds\n",
    "\n",
    "def count_dim_reduce(reduce_layer, x):\n",
    "    total_mults = total_adds = 0\n",
    "    # (dim_reduce)\n",
    "    # (0): ReLU\n",
    "    relu_layer = reduce_layer[0]\n",
    "    flop_mults, flop_adds = countReLU(x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = relu_layer(x)\n",
    "    # (1) Conv2d\n",
    "    conv_layer = reduce_layer[1]\n",
    "    flop_mults, flop_adds = count_Conv2d(conv_layer, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = conv_layer(x)\n",
    "    # (2) Batch Norm\n",
    "    norm_layer = reduce_layer[2]\n",
    "    x = norm_layer(x)\n",
    "\n",
    "    return total_mults, total_adds\n",
    "\n",
    "def count_Fit(fit_block, x, prev):\n",
    "    total_mults = total_adds = 0\n",
    "    if prev is None:\n",
    "        return total_mults, total_adds\n",
    "    \n",
    "    elif x.size(2) != prev.size(2):\n",
    "        # (relu): ReLU\n",
    "        flop_mults, flop_adds = countReLU(prev)\n",
    "        total_mults += flop_mults; total_adds += flop_adds\n",
    "        prev = fit_block.relu(prev)\n",
    "        \n",
    "        # (p1) Sequential\n",
    "        # (0) AvgPool2d\n",
    "        pool_layer = fit_block.p1[0]\n",
    "        flop_mults, flop_adds = count_AvgPool2d(pool_layer, prev)\n",
    "        total_mults += flop_mults; total_adds += flop_adds\n",
    "        p1 = pool_layer(prev)\n",
    "        # (1) Conv2d\n",
    "        conv_layer = fit_block.p1[1]\n",
    "        flop_mults, flop_adds = count_Conv2d(conv_layer, p1)\n",
    "        total_mults += flop_mults; total_adds += flop_adds\n",
    "        p1 = conv_layer(p1)\n",
    "        \n",
    "        # (p2) Sequential\n",
    "        # (0) ConstantPad2d\n",
    "        # (1) ConstantPad2d\n",
    "        pad_layer = fit_block.p2[0]\n",
    "        p2 = pad_layer(prev)\n",
    "        pad_layer = fit_block.p2[1]\n",
    "        p2 = pad_layer(p2)\n",
    "        # (2) AvgPool2d\n",
    "        pool_layer = fit_block.p2[2]\n",
    "        flop_mults, flop_adds = count_AvgPool2d(pool_layer, p2)\n",
    "        total_mults += flop_mults; total_adds += flop_adds\n",
    "        p2 = pool_layer(p2)\n",
    "        # (3) Conv2d\n",
    "        conv_layer = fit_block.p2[3]\n",
    "        flop_mults, flop_adds = count_Conv2d(conv_layer, p2)\n",
    "        total_mults += flop_mults; total_adds += flop_adds\n",
    "        p2 = conv_layer(p2)\n",
    "        # new prev is concatenated. No operations\n",
    "        prev = torch.cat([p1, p2], 1)\n",
    "        \n",
    "        # (bn) Batch Norm\n",
    "        norm_layer = fit_block.bn\n",
    "        prev = norm_layer(prev)\n",
    "        \n",
    "        return total_mults, total_adds\n",
    "        \n",
    "    else:\n",
    "        return count_dim_reduce(normalcell.fit.dim_reduce, prev)\n",
    "    \n",
    "def count_SeparableConv2d(sep_conv2d_layer, x):\n",
    "    total_mults = total_adds = 0\n",
    "    # depthwise\n",
    "    depth_layer = sep_conv2d_layer.depthwise\n",
    "    flop_mults, flop_adds = count_Conv2d(depth_layer, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = depth_layer(x)\n",
    "    # pointwise\n",
    "    point_layer = sep_conv2d_layer.pointwise\n",
    "    flop_mults, flop_adds = count_Conv2d(point_layer, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = point_layer(x)\n",
    "    \n",
    "    return total_mults, total_adds\n",
    "\n",
    "def count_SeparableBranch(separable_branch, x):\n",
    "    total_mults = total_adds = 0\n",
    "    \n",
    "    # (block1)\n",
    "    block1 = separable_branch.block1\n",
    "    # (0): ReLU\n",
    "    flop_mults, flop_adds = countReLU(x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = block1[0](x)\n",
    "    \n",
    "    # (1) Separable Conv2d\n",
    "    sep_conv2d_layer = block1[1]\n",
    "    flop_mults, flop_adds = count_SeparableConv2d(sep_conv2d_layer, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = sep_conv2d_layer(x)\n",
    "    \n",
    "    # (2) Batch Norm\n",
    "    norm_layer = block1[2]\n",
    "    x = norm_layer(x)\n",
    "    \n",
    "    # block2\n",
    "    block2 = separable_branch.block2\n",
    "    # (0): ReLU\n",
    "    flop_mults, flop_adds = countReLU(x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = block2[0](x)\n",
    "    \n",
    "    # (1) Separable Conv2d\n",
    "    sep_conv2d_layer = block2[1]\n",
    "    flop_mults, flop_adds = count_SeparableConv2d(sep_conv2d_layer, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x = sep_conv2d_layer(x)\n",
    "    \n",
    "    # (2) Batch Norm\n",
    "    norm_layer = block2[2]\n",
    "    x = norm_layer(x)\n",
    "    \n",
    "    return total_mults, total_adds\n",
    "\n",
    "def count_NormalCell(normalcell, x, prev):\n",
    "    total_mults = total_adds = 0\n",
    "    # run fit\n",
    "    fit_block = normalcell.fit\n",
    "    flop_mults, flop_adds = count_Fit(fit_block, x, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    prev = fit_block((x, prev))\n",
    "\n",
    "    # run dim_reduce\n",
    "    reduce_block  = normalcell.dem_reduce\n",
    "    flop_mults, flop_adds = count_dim_reduce(reduce_block, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    h = reduce_block(x)\n",
    "\n",
    "    # get x1\n",
    "    block1_left = normalcell.block1_left\n",
    "    flop_mults, flop_adds = count_SeparableBranch(block1_left, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    # block1_right is empty\n",
    "\n",
    "\n",
    "    # get x2\n",
    "    block2_left = normalcell.block2_left\n",
    "    flop_mults, flop_adds = count_SeparableBranch(block2_left, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    block2_right = normalcell.block2_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(block2_right, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    # get x3\n",
    "    block3_left = normalcell.block3_left\n",
    "    flop_mults, flop_adds = count_AvgPool2d(block3_left, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    # block3_right is empty\n",
    "\n",
    "\n",
    "    # get x4\n",
    "    block4_left = normalcell.block4_left\n",
    "    flop_mults, flop_adds = count_AvgPool2d(block4_left, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    block4_right = normalcell.block4_left\n",
    "    flop_mults, flop_adds = count_AvgPool2d(block4_left, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    # get x5\n",
    "    block5_left = normalcell.block5_left\n",
    "    flop_mults, flop_adds = count_SeparableBranch(block5_left, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    block5_right = normalcell.block5_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(block5_right, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    \n",
    "    return total_mults, total_adds\n",
    "\n",
    "def count_ReductionCell(reductioncell, x, prev):\n",
    "    total_mults = total_adds = 0\n",
    "    # run fit\n",
    "    fit_block = reductioncell.fit\n",
    "    flop_mults, flop_adds = count_Fit(fit_block, x, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    prev = fit_block((x, prev))\n",
    "\n",
    "    # run dim_reduce\n",
    "    reduce_block  = reductioncell.dim_reduce\n",
    "    flop_mults, flop_adds = count_dim_reduce(reduce_block, x)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    h = reduce_block(x)\n",
    "\n",
    "    # get layer1block1\n",
    "    layer1block1_left = reductioncell.layer1block1_left\n",
    "    flop_mults, flop_adds = count_SeparableBranch(layer1block1_left, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    layer1block1_right = reductioncell.layer1block1_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(layer1block1_right, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    layer1block1 = reductioncell.layer1block1_left(prev) + reductioncell.layer1block1_right(h)\n",
    "\n",
    "    # get layer1block2\n",
    "    # left is maxpool, so no flop\n",
    "    layer1block2_left = reductioncell.layer1block2_left\n",
    "\n",
    "    layer1block2_right = reductioncell.layer1block2_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(layer1block2_right, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    layer1block2 = reductioncell.layer1block2_left(h) + reductioncell.layer1block2_right(prev)\n",
    "\n",
    "    # get layer1block3\n",
    "    layer1block3_left = reductioncell.layer1block3_left\n",
    "    flop_mults, flop_adds = count_AvgPool2d(layer1block3_left, h)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    layer1block3_right = reductioncell.layer1block3_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(layer1block3_right, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    # get layer2block1\n",
    "    # left is maxpool, so no flop\n",
    "    layer1block3_right = reductioncell.layer1block3_right\n",
    "    flop_mults, flop_adds = count_SeparableBranch(layer1block3_right, layer1block1)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "\n",
    "    # get layer2block2\n",
    "    layer2block2_left = reductioncell.layer2block2_left\n",
    "    flop_mults, flop_adds = count_AvgPool2d(layer2block2_left, layer1block1)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    # layer2block2_right is just Sequential() so no FLOP\n",
    "    return total_mults, total_adds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem\n",
    "The stem for nasnet consists of one Conv2d followed by BachNorm2d Operation. We assume a [fused](https://tehnokv.com/posts/fusing-batchnorm-and-conv) conv + batchnorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mults: 1216512 and adds: 1171456 for stem layer.\n"
     ]
    }
   ],
   "source": [
    "total_mults = total_adds = 0\n",
    "x = torch.Tensor(1, 3, 32, 32).cuda()\n",
    "conv_layer = net.stem[0]\n",
    "\n",
    "# (1) Conv2D and (3) BatchNorm2d\n",
    "flop_mults, flop_adds = count_Conv2d(conv_layer, x)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x = net.stem[0](x)\n",
    "# assume fused conv layer at batch norm at inference\n",
    "x = net.stem[1](x)\n",
    "print('Total mults: ' + '{0}'.format(total_mults) + ' and adds: ' + '{0}'.format(total_adds) + ' for stem layer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell Layers\n",
    "Nasnet has several sequential and reduction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Cell (Layers 0 - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = None\n",
    "\n",
    "for i in range(4):\n",
    "    normalcell = net.cell_layers[i]\n",
    "    flop_mults, flop_adds = count_NormalCell(normalcell, x, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x, prev = normalcell((x, prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Cell (Layer 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductioncell = net.cell_layers[4]\n",
    "flop_mults, flop_adds = count_ReductionCell(reductioncell, x, prev)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x, prev = reductioncell((x, prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Cell (Layers 5 - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, 9):\n",
    "    normalcell = net.cell_layers[i]\n",
    "    flop_mults, flop_adds = count_NormalCell(normalcell, x, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x, prev = normalcell((x, prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Cell (Layer 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductioncell = net.cell_layers[9]\n",
    "flop_mults, flop_adds = count_ReductionCell(reductioncell, x, prev)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x, prev = reductioncell((x, prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Cell (Layers 10 - 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 14):\n",
    "    normalcell = net.cell_layers[i]\n",
    "    flop_mults, flop_adds = count_NormalCell(normalcell, x, prev)\n",
    "    total_mults += flop_mults; total_adds += flop_adds\n",
    "    x, prev = normalcell((x, prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu\n",
    "flop_mults, flop_adds = countReLU(x)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x = net.relu(x)\n",
    "\n",
    "# Adaptive Average Pooling 2D\n",
    "pooling_layer = net.avg\n",
    "flop_mults, flop_adds = count_AdaptiveAvgPool2d(pool_layer, x)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x = pooling_layer(x)\n",
    "\n",
    "# changing the view\n",
    "x = x.view(x.size(0), -1)\n",
    "\n",
    "# Adaptive Average Pooling 2D\n",
    "fc_layer = net.fc\n",
    "flop_mults, flop_adds = count_FullyConnected(fc_layer, x)\n",
    "total_mults += flop_mults; total_adds += flop_adds\n",
    "x = fc_layer(x)\n",
    "\n",
    "n_classes = 100\n",
    "assert x.shape[1] == n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mults: 5466162592 and adds: 5458841696 for nasnet.\n",
      "nasnet achieves an ops score of 0.7809\n"
     ]
    }
   ],
   "source": [
    "total_mults_freebie = total_mults / 2\n",
    "benchmark_ops = 10.49e9\n",
    "ops_score = (total_mults_freebie + total_adds) / benchmark_ops\n",
    "print('Total mults: ' + '{0}'.format(total_mults) + ' and adds: ' + '{0}'.format(total_adds) + ' for nasnet.')\n",
    "print(args.net + ' achieves an ops score of ' + '{0:.4}'.format(ops_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 0.85245598\n"
     ]
    }
   ],
   "source": [
    "print('Final Score: ' + '{0:.8}'.format(ops_score + parameter_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
